# Fidlity Bench


## ğŸš€ Quick Start
```
cd fidbench
pip install -e .
pip install -r requirement.txt
```

## æ·»åŠ æ–°å‹ç¼©æ–¹æ³•

1. ç¬¬ä¸€æ­¥ï¼Œåœ¨`fidbench/modifiers/`ä¸‹æ·»åŠ å¯¹åº”æ–¹æ³•çš„æ–‡ä»¶

   éœ€è¦ç»§æ‰¿`Modifier`ç±»ï¼Œä¸”å¿…é¡»åœ¨initæ–¹æ³•ä¸­æ‹¥æœ‰4ä¸ªè¾“å…¥å‚æ•°
   * model, æ¨¡å‹æœ¬èº«
   * save_ckp, load_ckpï¼Œè¿™ä¸¤ä¸ªä¸ç”¨ç®¡
   * config, é…ç½®æ–‡ä»¶ï¼Œdictå­—å…¸ï¼Œè‡ªåŠ¨ç»™å€¼ï¼Œå¯¹åº”configæ–‡ä»¶å¤¹ä¸­çš„é…ç½®æ–‡ä»¶

   ä¾‹å¦‚ï¼š
   ```python
   from ..modifier import Modifier
   import torch

   class XXXMethod(Modifier):

      def __init__(self, model, save_ckp=None, load_ckp=None, config=None):

         # å¦‚æœä¸éœ€è¦é…ç½®æ–‡ä»¶ï¼Œåˆ™ä¸éœ€è¦ä¸‹é¢å‡ è¡Œ
         self.get_conf(config)
         a = self.conf['a']
         b = self.conf['b']
         ...

         super().__init__(model, save_ckp, load_ckp)


      # ä¸ç”¨ç®¡ï¼Œreturn [] å°±è¡Œ
      def ft_params(self):
         return []

      # ä¸ç”¨ç®¡ï¼Œpasså°±è¡Œ
      def reset(self):
         pass

      # è¿™ä¸ªå¿…é¡»æœ‰ï¼Œè¾“å…¥ä¸ºp_ids (prefilling tokens) ä»¥åŠ g_ids (generation tokens)ï¼Œåˆ†åˆ«å¯¹åº”ä¸Šä¸‹æ–‡ï¼Œä»¥åŠbase modelç”Ÿæˆçš„é‚£éƒ¨åˆ†
      @torch.no_grad()
      def compute_accuracy(self, p_ids, g_ids):

         assert p_ids.shape[0] == 1, 'only support batch size 1'
         assert p_ids.ndim == 2 and g_ids.ndim == 2

         device = next(iter(self.model.parameters())).device
         p_ids, g_ids = p_ids.to(device), g_ids.to(device)

         # pre-fill, å¯èƒ½è¦å˜åŠ¨
         output = self.model(input_ids=p_ids)
         kv_cache = output.past_key_values

         acc1, acc5 = 0, 0
         turns = g_ids.shape[-1] - 1

         for tok, label in zip(
                  torch.chunk(g_ids[:, :-1], turns, dim=-1), 
                  torch.chunk(g_ids[:, 1:], turns, dim=-1)):

               # generation, å¯èƒ½è¦å˜åŠ¨
               output = self.model(input_ids=tok, kv_cache=kv_cache)
               logits, kv_cache = output.logits, output.past_key_values

               label = label.ravel().item()
               next_1 = logits.argmax(dim=-1).ravel().item()
               next_5 = logits.topk(k=5, dim=-1).indices.ravel().tolist()

               acc1 += next_1 == label
               acc5 += label in next_5

         acc1 /= turns
         acc5 /= turns

         return acc1, acc5

   # è¿™ä¸ªä¹Ÿå¿…é¡»æœ‰ï¼Œè¾“å…¥ä¸ºinput_ids
   @torch.no_grad()
   def compute_ppl(self, input_ids):
      assert input_ids.shape[0] == 1, 'only support batch size 1'
      assert input_ids.ndim == 2

      device = next(iter(self.model.parameters())).device
      input_ids = input_ids.to(device)

      output = self.model(input_ids=input_ids)
      logits = output.logits

      shift_logits = logits[:, :-1, :].contiguous()
      shift_labels = input_ids[:, 1:].contiguous()

      loss = F.cross_entropy(
         shift_logits.view(-1, shift_logits.size(-1)),
         shift_labels.view(-1),
         reduction='mean'
      )

      ppl = torch.exp(loss).item()

      return ppl
   ```


2. ä¿®æ”¹`fidbench/modifiers/__init__.py`, æ³¨å†ŒXXXMethodç±»
   ```python
   def get_modifier(method: str):

    if method == "origin":
        from .origin import Origin
        return Origin
    
    elif method == 'topk-llama':
        from .topk import Topk
        return Topk

    elif method == 'topk-qwen':
        from .topk import Topk
        return Topk

    # æ·»åŠ åœ¨äº†è¿™é‡Œ!!!
    elif method == 'xxxmethod':
        from .xxxmethod import XXXMethod
        return XXXMethod
    
    else:
        raise NotImplementedError(method)
   ```

3. åœ¨runsæ–‡ä»¶å¤¹ä¸­æ·»åŠ å¯¹åº”çš„è¿è¡Œé…ç½®æ–‡ä»¶ `llama3.1-8b-instruct-xxxmethod.json`

   æ³¨æ„åå­—å¿…é¡»æ˜¯ `{model_name}-{method}.json`æ ¼å¼çš„ï¼Œæ–¹ä¾¿æ‰¹å¤„ç†
   ```json
   {
      "model": {
         "model_name": "/mnt/petrelfs/share_data/ai4good_shared/models/meta-llama/Llama-3.1-8B-Instruct",
         "model_dtype": "bf16",
         "model_method": "xxxmethod", # å¯¹åº”ä¸Šä¸€æ­¥çš„æ³¨å†Œ
         "save_ckp": null,
         "load_ckp": null,
         "config": "config/xxxmethod.json" | null, # è¿™ä¸ªæ˜¯é…ç½®æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰é…ç½®ï¼Œåˆ™è®¾ç½®ä¸ºnull
         "device_map": null,
         "max_length": 131072
      }
   }
   ```

4. æµ‹è¯•accuracy

   åœ¨`pred.sh`ä¸­æ·»åŠ xxxmethodæ–¹æ³•
   ```bash
      bases=(
      "llama3.1-8b-instruct"
   )

   methods=(
      "topk"
      "xxxmethod" # æ–°å¢åŠ 
   )
   ```

5. æµ‹è¯•perplexity

   åœ¨`perplexity.sh`ä¸­æ·»åŠ xxxmethodæ–¹æ³•
   ```bash
   bases=(
      "llama3.1-8b-instruct"
   )

   methods=(
      "topk"
      "xxxmethod" # æ–°å¢åŠ 
   )
   
   ```


## å·²æœ‰æ–¹æ³•

1. TransMLA
   ```bash
   conda create -n transmla python=3.12.8
   conda activate transmla
   ```

   ```bash
   cd thirdparty/TransMLA
   pip install -r requirements.txt
   ```

   è½¬æ¢æ¨¡å‹å‚æ•°
   ```
   bash scripts/convert/qwen2.5-7B-Instruct.sh
   ```

2. SageAttention2
   ```bash
   conda create -n transmla
   conda activate sageattn
   ```
   ```bash
   cd thirdparty/SageAttention
   python setup.py install
   ```

3. FlashAttention FP8
   Enviromental requiremnt: `CUDA==12.8`
   ```bash
   conda create -n transmla
   conda activate fp8
   ```
   ```
   cd thirdparty/flash-attention
   git submodule update --init csrc/cutlass
   cd hopper
   python setup.py install
   ```

4. SpargeAttention
   Environment requirements:
   `python>=3.9`, `torch>=2.3.0`, `CUDA>=12.8` for Blackwell, `>=12.4` for fp8 support on Ada, `>=12.3` for fp8 support on Hopper, `>=12.0` for Ampere
   ```bash
   conda create -n spargeattn --clone transmla
   conda activate spargeattn
   ```

   ```bash
   cd thirdparty/SpargeAttn
   pip install ninja
   python setup.py install
   ```

5. Medusa
   Data Prepare
   ```bash
   cd thirdparty/Medusa
   mkdir data
   huggingface-cli download Aeala/ShareGPT_Vicuna_unfiltered --repo-type dataset --include ShareGPT_V4.3_unfiltered_cleaned_sp
lit.json
   ```

   è®­ç»ƒ
   ```
   pip install transformers==4.37.2
   mv /mnt/petrelfs/liwenhao/fidbench/thirdparty/Medusa/medusa/train/train_legacy.py /mnt/petrelfs/liwenhao/fidbench/thirdparty/Medusa/medusa/train/train.py
   ```

   medusa_model.pyé‡Œé¢è¦æ”¹åŠ¨ LlamaAttentioné‡Œçš„
   ```bash
   self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)
   self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
   self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
   self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
   ```

   åœ¨medusa_model.pyçš„æœ€æœ«å°¾å†™æ­»ä½¿ç”¨LLaMAç»“æ„è¿›è¡Œè¯»å–
   ```
   class MedusaModel():
    @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path,
        *args,
        **kwargs,
    ):
        # Manually load config to ensure that the medusa_num_heads parameter is loaded
        try:
            config = AutoConfig.from_pretrained(pretrained_model_name_or_path)
        except:
            # MEDUSA-v0.1 load
            config = MedusaConfig.from_pretrained(pretrained_model_name_or_path)
            base_model_config = AutoConfig.from_pretrained(config.base_model_name_or_path)
            config.model_type = base_model_config.model_type

        return MedusaModelLlama.from_pretrained(
            pretrained_model_name_or_path,
            *args,
            **kwargs,
        )
   ```

   medusa_model.pyé‡Œé¢å»æ‰æ‰€æœ‰çš„self.pretrain_tpç›¸å…³çš„
   

6. Dejavu
   åˆ›å»ºæ–°çš„ç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
   ```bash
   pip install cupy-cuda12x
   python -m cupyx.tools.install_library --cuda 12.x --library nccl
   ```