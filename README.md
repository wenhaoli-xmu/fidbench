# Fidlity Bench


## ğŸš€ Quick Start
```
cd fidbench
pip install -e .
pip install -r requirement.txt
```

## æ·»åŠ æ–°å‹ç¼©æ–¹æ³•

1. ç¬¬ä¸€æ­¥ï¼Œåœ¨`fidbench/modifiers/`ä¸‹æ·»åŠ å¯¹åº”æ–¹æ³•çš„æ–‡ä»¶

   éœ€è¦ç»§æ‰¿`Modifier`ç±»ï¼Œä¸”å¿…é¡»åœ¨initæ–¹æ³•ä¸­æ‹¥æœ‰4ä¸ªè¾“å…¥å‚æ•°
   * model, æ¨¡å‹æœ¬èº«
   * save_ckp, load_ckpï¼Œè¿™ä¸¤ä¸ªä¸ç”¨ç®¡
   * config, é…ç½®æ–‡ä»¶ï¼Œdictå­—å…¸ï¼Œè‡ªåŠ¨ç»™å€¼ï¼Œå¯¹åº”configæ–‡ä»¶å¤¹ä¸­çš„é…ç½®æ–‡ä»¶

   ä¾‹å¦‚ï¼š
   ```python
   from ..modifier import Modifier
   import torch

   class XXXMethod(Modifier):

      def __init__(self, model, save_ckp=None, load_ckp=None, config=None):

         # å¦‚æœä¸éœ€è¦é…ç½®æ–‡ä»¶ï¼Œåˆ™ä¸éœ€è¦ä¸‹é¢å‡ è¡Œ
         self.get_conf(config)
         a = self.conf['a']
         b = self.conf['b']
         ...

         super().__init__(model, save_ckp, load_ckp)


      # ä¸ç”¨ç®¡ï¼Œreturn [] å°±è¡Œ
      def ft_params(self):
         return []

      # ä¸ç”¨ç®¡ï¼Œpasså°±è¡Œ
      def reset(self):
         pass

      # è¿™ä¸ªå¿…é¡»æœ‰ï¼Œè¾“å…¥ä¸ºp_ids (prefilling tokens) ä»¥åŠ g_ids (generation tokens)ï¼Œåˆ†åˆ«å¯¹åº”ä¸Šä¸‹æ–‡ï¼Œä»¥åŠbase modelç”Ÿæˆçš„é‚£éƒ¨åˆ†
      @torch.no_grad()
      def compute_accuracy(self, p_ids, g_ids):

         assert p_ids.shape[0] == 1, 'only support batch size 1'
         assert p_ids.ndim == 2 and g_ids.ndim == 2

         device = next(iter(self.model.parameters())).device
         p_ids, g_ids = p_ids.to(device), g_ids.to(device)

         # pre-fill, å¯èƒ½è¦å˜åŠ¨
         output = self.model(input_ids=p_ids)
         kv_cache = output.past_key_values

         acc1, acc5 = 0, 0
         turns = g_ids.shape[-1] - 1

         for tok, label in zip(
                  torch.chunk(g_ids[:, :-1], turns, dim=-1), 
                  torch.chunk(g_ids[:, 1:], turns, dim=-1)):

               # generation, å¯èƒ½è¦å˜åŠ¨
               output = self.model(input_ids=tok, kv_cache=kv_cache)
               logits, kv_cache = output.logits, output.past_key_values

               label = label.ravel().item()
               next_1 = logits.argmax(dim=-1).ravel().item()
               next_5 = logits.topk(k=5, dim=-1).indices.ravel().tolist()

               acc1 += next_1 == label
               acc5 += label in next_5

         acc1 /= turns
         acc5 /= turns

         return acc1, acc5
   ```


2. ä¿®æ”¹`fidbench/modifiers/__init__.py`, æ³¨å†ŒXXXMethodç±»
   ```python
   def get_modifier(method: str):

    if method == "origin":
        from .origin import Origin
        return Origin
    
    elif method == 'topk-llama':
        from .topk import Topk
        return Topk

    elif method == 'topk-qwen':
        from .topk import Topk
        return Topk

    # æ·»åŠ åœ¨äº†è¿™é‡Œ!!!
    elif method == 'xxxmethod':
        from .xxxmethod import XXXMethod
        return XXXMethod
    
    else:
        raise NotImplementedError(method)
   ```

3. åœ¨runsæ–‡ä»¶å¤¹ä¸­æ·»åŠ å¯¹åº”çš„è¿è¡Œé…ç½®æ–‡ä»¶ `llama3.1-8b-instruct-xxxmethod.json`

   æ³¨æ„åå­—å¿…é¡»æ˜¯ `{model_name}-{method}.json`æ ¼å¼çš„ï¼Œæ–¹ä¾¿æ‰¹å¤„ç†
   ```json
   {
      "model": {
         "model_name": "/mnt/petrelfs/share_data/ai4good_shared/models/meta-llama/Llama-3.1-8B-Instruct",
         "model_dtype": "bf16",
         "model_method": "xxxmethod", # å¯¹åº”ä¸Šä¸€æ­¥çš„æ³¨å†Œ
         "save_ckp": null,
         "load_ckp": null,
         "config": "config/xxxmethod.json" | null, # è¿™ä¸ªæ˜¯é…ç½®æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰é…ç½®ï¼Œåˆ™è®¾ç½®ä¸ºnull
         "device_map": null,
         "max_length": 131072
      }
   }
   ```

4. è¿è¡Œ

   åœ¨`pred.sh`ä¸­æ·»åŠ xxxmethodæ–¹æ³•
   ```bash
      bases=(
      "llama3.1-8b-instruct"
   )

   methods=(
      "topk"
      "xxxmethod" # æ–°å¢åŠ 
   )
   ```