# Fidlity Bench


## ğŸš€ Quick Start
```
cd fidbench
pip install -e .
pip install -r requirement.txt
```

## æ·»åŠ æ–°å‹ç¼©æ–¹æ³•

1. ç¬¬ä¸€æ­¥ï¼Œåœ¨`fidbench/modifiers/`ä¸‹æ·»åŠ å¯¹åº”æ–¹æ³•çš„æ–‡ä»¶

   éœ€è¦ç»§æ‰¿`Modifier`ç±»ï¼Œä¸”å¿…é¡»åœ¨initæ–¹æ³•ä¸­æ‹¥æœ‰4ä¸ªè¾“å…¥å‚æ•°
   * model, æ¨¡å‹æœ¬èº«
   * save_ckp, load_ckpï¼Œè¿™ä¸¤ä¸ªä¸ç”¨ç®¡
   * config, é…ç½®æ–‡ä»¶ï¼Œdictå­—å…¸ï¼Œè‡ªåŠ¨ç»™å€¼ï¼Œå¯¹åº”configæ–‡ä»¶å¤¹ä¸­çš„é…ç½®æ–‡ä»¶

   ä¾‹å¦‚ï¼š
   ```python
   from ..modifier import Modifier
   import torch

   class GreedyGeneration(Modifier):

      def __init__(self, model, save_ckp=None, load_ckp=None, config=None):

         # å¦‚æœä¸éœ€è¦é…ç½®æ–‡ä»¶ï¼Œåˆ™ä¸éœ€è¦ä¸‹é¢å‡ è¡Œ
         self.get_conf(config)
         a = self.conf['a']
         b = self.conf['b']
         ...

         super().__init__(model, save_ckp, load_ckp)


      # ä¸ç”¨ç®¡ï¼Œreturn [] å°±è¡Œ
      def ft_params(self):
         return []

      # ä¸ç”¨ç®¡ï¼Œpasså°±è¡Œ
      def reset(self):
         pass

      # è¿™ä¸ªå¿…é¡»æœ‰ï¼Œè¾“å…¥å‚æ•°å¿…é¡»ä¿è¯è‡³å°‘æœ‰input_ids, max_new_tokensä»¥åŠeos_token_idä¸‰ä¸ª
      @torch.no_grad()
      def generate(self, input_ids, max_new_tokens=128, eos_token_id=2):

         if isinstance(input_ids, list):
               input_ids = torch.tensor(input_ids, dtype=torch.int64)[None, :]

         if input_ids.ndim == 3:
               input_ids = input_ids.flatten(0,1)

         # put the tensor on to the model's device
         device = next(iter(self.model.parameters())).device
         input_ids = input_ids.to(device)

         # fake prefilling
         output = self.model(input_ids=input_ids[:, :128])
         logits, past_key_values = output.logits, output.past_key_values

         new_tok = logits.argmax(dim=-1)
         new_ids = [new_tok]

         while len(new_ids) < max_new_tokens:
               output = self.model.model(input_ids=new_tok, past_key_values=past_key_values)
               logits, past_key_values = output.logits, output.past_key_values

               new_tok = logits.argmax(dim=-1)
               if new_tok.ravel().item() in eos_token_id:
                  break

               new_ids.append(new_tok.to(input_ids.device))

         return torch.cat([input_ids, *new_ids], dim=-1)
   ```


2. ä¿®æ”¹`fidbench/modifiers/__init__.py`, æ³¨å†Œ GreedyGenerationç±»
   ```python
   def get_modifier(method: str):

    if method == "origin":
        from .origin import Origin
        return Origin
    
    if method == 'greedy':
        from .greedy import Greedy
        return Greedy
    
    elif method == 'topk-llama':
        from .topk import Topk
        return Topk

    elif method == 'topk-qwen':
        from .topk import Topk
        return Topk

    # æ·»åŠ åœ¨äº†è¿™é‡Œ!!!
    elif method == 'greedy-gen':
        from .greedy_gen import GreedyGeneration
        return GreedyGeneration
    
    else:
        raise NotImplementedError(method)
   ```

3. åœ¨runsæ–‡ä»¶å¤¹ä¸­æ·»åŠ å¯¹åº”çš„è¿è¡Œé…ç½®æ–‡ä»¶ `llama3.1-8b-instruct-greedy_gen.json`

   æ³¨æ„åå­—å¿…é¡»æ˜¯ `{model_name}-{method}.json`æ ¼å¼çš„ï¼Œæ–¹ä¾¿æ‰¹å¤„ç†
   ```python
   {
      "model": {
         "model_name": "/mnt/petrelfs/share_data/ai4good_shared/models/meta-llama/Llama-3.1-8B-Instruct",
         "model_dtype": "bf16",
         "model_method": "greedy-gen", # å¯¹åº”ä¸Šä¸€æ­¥çš„æ³¨å†Œ
         "save_ckp": null,
         "load_ckp": null,
         "config": "config/greedy-gen.json", # è¿™ä¸ªæ˜¯é…ç½®æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰é…ç½®ï¼Œåˆ™è®¾ç½®ä¸ºnull
         "device_map": null,
         "max_length": 131072
      },

      # è¿™ä¸ªå¯ä»¥æ ¹æ®éœ€è¦æ¥è®¾ç½®ï¼Œä¼šè‡ªåŠ¨è¢«é€è¿›generation()æ–¹æ³•ä¸­ä½œä¸ºé¢å¤–çš„å‚æ•°
      "generation_kwargs": {}
   }
   ```

4. è¿è¡Œ

   åœ¨`pred.sh`ä¸­æ·»åŠ greedy_genæ–¹æ³•
   ```bash
      bases=(
      "llama3.1-8b-instruct"
   )

   methods=(
      "topk"
      "greedy_gen" # æ–°å¢åŠ 
   )
   ```